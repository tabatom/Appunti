%----------------------------------------
%Materiale rilasciato sotto licenza Creative Commons BY-NC-SA 4.0

%È consentita la distribuzione e la modifica sotto questa licenza, per scopi non commerciali

%http://creativecommons.org/licenses/by-nc-sa/4.0/deed.it

%Tommaso Tabarelli
%Ottobre 2018
%-----------------------------------------
\documentclass[12pt, english, a4paper]{book}
\usepackage{mathptmx}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{enumerate}

\usepackage[margin=2cm]{geometry}


\usepackage[pdftitle={Models of Theorethical Physics},pdfauthor={Tommaso Tabarelli}]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=magenta,
    linktoc=all
}


\newtheorem{theorem}{Theorem}[section]


\newcommand{\bra}[1]{\left\langle #1 \right |}
\newcommand{\ket}[1]{\left| #1 \right \rangle}
\newcommand{\braket}[2]{\left\langle #1 | #2 \right\rangle}
\newcommand{\vmed}[1]{\left \langle #1 \right \rangle}
\newcommand{\tr}{\operatorname{Tr}}

\newcommand{\Rarr}{\hspace{0.5cm} \mathbb{\Rightarrow} \hspace{0.5cm}}
\newcommand{\parder}[2]{\frac{\partial^ #2}{\partial #1 ^ #2}}
\newcommand{\partiald}[1]{\frac{\partial}{\partial #1}}




\begin{document}


\begin{titlepage}

\begin{center}
\LARGE{Università degli Studi di Padova}\\
\line(1,0){450}\\
\vspace{10em}
\Huge{\textsc{\textbf{Models of Theoretical Physics}}}\\
\vspace{4em}
\LARGE{\textsc{di}}\\
\vspace{3em}
\huge{\textsc{Tommaso Tabarelli}}\\
\vspace{10em}
\LARGE{October 2018}\\
\line(1,0){450}
\end{center}

\end{titlepage}
\pagenumbering{roman}
\let\cleardoublepage\clearpage

\thispagestyle{empty}
\null
\vfill
\begin{center}
\small{This file is released under \href{http://creativecommons.org/licenses/by-sa/4.0/}{\color{blue}Creative Commons \emph{Attribution-ShareAlike 4.0 International} license}.}
\begin{figure}[H]
\centering
%\href{http://creativecommons.org/licenses/by-nc-sa/4.0/deed.it}{\includegraphics[scale=0.15]{by-sa.png}}
\end{figure}
\vspace{0.5em}
This means that this material can be freely modified and redistributed, as long as it is released under the same license, and proper credit is given to the original author.\\

\vspace{2em}
\scriptsize{October 2018}
\end{center}

\newpage
\thispagestyle{empty}
\chapter*{Introduction}
The aim of this document is to collect the notes of \textit{Models of Theoretical Physics} course, held by professors Marco Baiesi and Amos Maritan for the "Physics of data" curriculum in the academic year 2018-2019 (which is the first year of this new curriculum), to have them written in a neater way.

As just told, this document is far from pretendig to be perfect and his goal is to help studying in a neater and better way. For this reason, there may be some errors among it.

I have also decided to release this document together with its \LaTeX{} source code under the \href{http://creativecommons.org/licenses/by-sa/4.0/}{\color{blue}Creative Commons \emph{Attribution-ShareAlike 4.0 International}} license. In brief this means that this document can be freely modified and redistributed, provided that it is released again under the same license and that proper credit is given to the original author.\\

I have tried to be as much ruthless as possible in finding and correcting errors and mistakes, and I apologize if some have survived.\\

I hope that the overall result will anyway be satisfactory.

\vspace{2em}

\hfill \emph{Padua, October 2018}

\hfill \emph{Tommaso Tabarelli}

\newpage
\thispagestyle{empty}
\let\cleardoublepage\clearpage

\tableofcontents
\pagenumbering{arabic}

\chapter{Methods to compute usefull integrals}

\hspace*{\fill} (02/10/2018) \\ \\
In this course we will use many kind of particular integrals. For this reason, in this first chapter we will see a brief introduction to all we need to calculate them.

\section{Gaussian integrals}

Let us consider the following integral:\\

\begin{equation}\label{Gaussian Integral}
Z(A) = \int d^{n}x \cdot e^{ -A_2(\vec{x}) }
\end{equation}

where $Z(A)$ is the integral we want to calculate, $A_2(\vec{x})$ is a x quadratic form as the following:

\begin{equation}
A_2(\vec{x}) = \frac{1}{2} \sum\limits_{i,j = 1}^{n} x_i A_{ij} x_j
\end{equation}

Here $A$ is a matrix that can be identified as a metric matrix.
In the case we consider $A$ is symmetric with (generally) complex coefficients; furthermore, it has non-negative real parts and non-vanishing eigenvalues $a_i$.

$$ Re(a_i) \geq 0 \hspace{1cm} \mathrm{and} \hspace{1cm}  a_i \neq 0_{\mathbb{C}} $$

(Note: if these conditions are not true the integral would be divergent...).\\

Let us bring $A \in \mathbb{R}$. In this case it can be digonalized with orthogonal transformation $O$, for which it holds:

$$ \sum\limits_{i}^{} O_{ij} x_j = x'_j \hspace{1cm} \mathrm{and} \hspace{1cm} |det(O)| = 1 $$

This implies that the Jacobian matrix of the transformation $J$ (which is $O$ itself) has $|det(J)| = 1$: so when we use it to change coordinates in the integral, we can say that "it has no effect" (we multiply by 1).
Furthermore, with that transformation the non-diagonal coefficients of the new matrix become all 0 (the matrix after the transformation is diagonal), so the new coordinates $x'_j$ are indipendents: this mean that we can evaluate $Z(A)$ by dividing it in the integrals of every single $x'_j$.\\
We obtain:

\begin{equation}\label{Gaussian integral result}
Z(A) = (2\pi)^{ \frac{n}{2} } \hspace{1mm} \prod\limits_{i=1}^{n} \hspace{1mm} a_i^{ -\frac{1}{2} } = (2\pi)^{ \frac{n}{2} } \hspace{1mm} (det(A))^{ -\frac{1}{2} }
\end{equation}

(Note that for a diagonal matrix the product of the eigenvalues is equal to the determinant. Moreover, for Binet's theorem it holds $det(A \cdot O) = det(A) \cdot det(O)$ (true if $A$ and $O$ are square matrix with same dimensions)).\\
(Note: I think that what just written in the previous note modifies the conditions on $det(O)$, which I think would be $det(O) = +1$ and not $|det(O)| = 1 $ otherwise the last formula is meaningless.\\
This have to be investigated.\\
However, the important key is that the integral with that tranformation does not become divergent.)\\

Since \eqref{Gaussian Integral} and $det(A)$ are analytic functions of $A$'s coefficients, we can extend \eqref{Gaussian integral result} to $\mathbb{C}$. \\


\textcolor{blue}
{ Example: $$ A = \left(
\begin{matrix}
3 & -1 \\
-1 & 3
\end{matrix}
\right) \Rarr (3-\lambda)^2 - 1 = 0 $$
$$ \mathbb{\rightarrow} \hspace{0.5cm} \lambda_{1,2} = \frac{3+\sqrt{9-8}}{2} \Rarr \begin{matrix}
\lambda_1 = 2 \\ \lambda_2 = 4
\end{matrix}
$$
$$ Z(A) = 2\pi \frac{1}{ \sqrt{4 \cdot 2} } = \sqrt{ \frac{ \pi } {2} } $$ \\ \\ }

\section{More general integrals}

Let us try to generalize the previous agruments for a more general kind of integral:

\begin{equation}\label{Gaussian Integral generalized}
Z(A) = \int d^{n}x \cdot e^{ -A_2(\vec{x}) + \vec{b} \cdot \vec{x} }
\end{equation}

With $ \vec{b} \cdot \vec{x} = \sum\limits_{i=1}^{n}b_i x_i $. We want to have the integral in the form of \eqref{Gaussian Integral} so we expand the exponent near his maximum $x^*$. \\
To find $x^*$ (signs are already changed):
$$ \frac{\partial}{\partial x_i} (A_2(x) - \vec{b} \cdot \vec{x}) = 0 \Rarr \sum\limits_{j} A_{ij} x_j = b_i $$

(Note: $A_2 = A/2$: in the derivative the factor 1/2 goes away because we are derivating a quadratic form (if one does the calculation, it can be seen easy).)

So the solution is:

$$ x^* = \sum\limits_{i} (A^{-1})_{ij} b_j $$

Now we change variable $ \vec{x} \Rarr \vec{y} $ in order to have the integral in the form of \eqref{Gaussian Integral}. $\vec{y}$ is the deviation from the min of the exponential:

$$ \vec{y} = \vec{x} - \vec{x^*} \Rarr x_i = x^*_i + y_i $$

$$ \Rarr -A_2 (\vec{x}) + \vec{b} \cdot \vec{x} = w_2(\vec{b}) - A_2 (\vec{y}) $$

Notice that $ w_2(\vec{b})$ is a costant, so it can be eventually take outside the integral.

$$ w_2(\vec{b}) = \frac{1}{2} \sum\limits_{i,j=0}^{n} b_i (A^{-1})_{ij} b_j = \frac{1}{2} \vec{b} \cdot \vec{x^*} $$

With those changes, we can write:

\begin{equation}\label{Gaussian Integral x_to_y}
Z(A, \vec{b}) = e^{w_2(\vec{b})} \int d^{n}y \cdot e^{ -A_2(\vec{y})} = e^{w_2(\vec{b})} (2\pi)^{\frac{n}{2}} (det(A))^{-\frac{1}{2}}
\end{equation} \\

\textcolor{blue}
{ Example (as before) $$ A = \left(
\begin{matrix}
3 & -1 \\
-1 & 3
\end{matrix}
\right) \hspace{0.5cm} \mathrm{and} \hspace{0.5cm} b = \left(
\begin{matrix}
1 \\
0
\end{matrix} \right)
$$
$$ Z(A) = \int \int e^{- \frac{1}{2} (x,y) A \left( \begin{smallmatrix}
x \\
y
\end{smallmatrix} \right) +  \left( \begin{smallmatrix}
x \\
0
\end{smallmatrix} \right)} dxdy $$
$$ A^{-1} = \frac{1}{det(A)} \left(
\begin{matrix}
A_{22} & -A_{12} \\
-A_{21} & A_{11}
\end{matrix}
\right) = \frac{1}{8} \left(
\begin{matrix}
3 & 1 \\
1 & 3
\end{matrix}
\right) $$
$$ w_2(\vec{b}) = \frac{1}{2} (1, 0) \frac{1}{8} \left(
\begin{matrix}
3 & 1 \\
1 & 3
\end{matrix}
\right) \left(
\begin{matrix}
1 \\
0
\end{matrix}
\right) = \frac{1}{16} (1, 0) \left(
\begin{matrix}
3 \\
1
\end{matrix}
\right) = \frac{3}{16}
$$
$$ \Rarr Z(A, \vec{b}) = e^{\frac{3}{16}} Z(A) = e^{\frac{3}{16}} \frac{\pi}{\sqrt{2}} $$} \\

\section{Gaussian expectation values}

We want now to explore briefly how the expactation values of polynomials with a gaussian distribution works.

\begin{equation}\label{Average Gaussian Polynomial}
\langle x_{k_1}, x_{k_2}, ... , x_{k_m} \rangle = \underbrace{Z^{-1}(A, \vec{0})}_{Normalization factor} \int d^n x x_{k_1}, x_{k_2}, ... , x_{k_m} e^{-A_2(\vec{x}) }
\end{equation}

Where the "normalization factor" is that for which $ \langle 1 \rangle = 1 $.

Note: since here we use angular brackets to denote an average made with gaussian weights.\\

From \eqref{Gaussian Integral generalized} we have:

$$ \frac{\partial}{\partial b_k} Z(A, \vec{b}) = \int d^n x \hspace{1mm} x_k \hspace{1mm} e^{-A_2(\vec{x}) + \vec{b} \cdot \vec{x}} $$

In this sense we say that $\vec{b}$ is "completed" to $\vec{x}$ and that Z is the generating function.
In the case of only 1 variable average note that if $\vec{b}=0$ the result is 0, as one expect for a gaussian variable.

Generalizing \eqref{Average Gaussian Polynomial} using what just shown, we obtain:

\begin{equation}\label{Average Gaussian Partial}
\langle x_{k_1}, x_{k_2}, ... , x_{k_l} \rangle = (2\pi)^{-\frac{n}{2}} (detA)^{\frac{1}{2}} \left[ \frac{\partial}{\partial b_{k_1}} \frac{\partial}{\partial b_{k_2}} ... \frac{\partial}{\partial b_{k_m}} Z(A, \vec{b}) \right]_{b=0}
\end{equation} \\ \\

NOT SURE ARGUMENT \\
We can also notice that if we want to evaluate the expectation value of a power series of x $ \langle F(x) \rangle $ we can write the following, thanks to the linearity of the integral (assuming that x is not a vector):

$$ \langle F(x) \rangle = ... \left[ F \left( \frac{\partial}{\partial b} \right) e^{w_2(b)} \right]_{b=0} $$

The "..." indicates that every power of x in F(x) has its own normalization factor. \\ \\

From \eqref{Average Gaussian Partial} we can obtain one form of Wick's theorem.\\
If there is one $ \frac{\partial}{\partial b_i} $ in \eqref{Average Gaussian Partial}, it pulls down some $b_j$ because $ w_2(\vec{b}) = \frac{1}{2} \sum\limits_{i,j=0}^{n} b_i (A^{-1})_{ij} b_j $. Without another $ \frac{\partial}{\partial b_k} $ (also with $ k \neq i$ ), setting $ \vec{b} = \vec{0} $ makes all components $ b_j \to 0 $ in $ \langle ... \rangle $. \\

$$\lim_{b_1 \to 0}  \langle b1, ... \rangle \to 0 $$ \\

$\Rightarrow$ Another $ \frac{\partial}{\partial b_k} $ is needed to ensure the average does not tend to 0. \\
$ \Rightarrow l $ index in \eqref{Average Gaussian Partial} must be even.

\begin{theorem}[Wick's theorem]\label{Wick's theorem}
Using arguments just above, for every $ k_p k_q $ from the list $ \lbrace k_1, k_2, ... , k_l \rbrace $, with associated $ (A^{-1})_{k_p k_q} $, the average of $l$ (multiplied) variables  can be calculated as following: \\

\begin{align*}
\left\langle x_{k_1}, x_{k_2}, ... , x_{k_l} \right\rangle & = \sum\limits_{all \hspace{1mm} possible \hspace{1mm} pairings \hspace{1mm} P \hspace{1mm} of \hspace{1mm} \lbrace k_1, ... , k_l \rbrace } A^{-1}_{k_{P_1} k_{P_2}} ... A^{-1}_{k_{P_{l-1}} k_{P_l}} \\
& = \sum\limits_{all \hspace{1mm} possible \hspace{1mm} pairings \hspace{1mm} P \hspace{1mm} of \hspace{1mm} \lbrace k_1, ... , k_l \rbrace } \langle x_{k_{P_1}} x_{k_{P_2}} \rangle \cdot ... \cdot \langle x_{k_{P_{l-1}}} x_{k_{P_l}} \rangle
\end{align*}

\end{theorem}

\textcolor{blue}
{Example: \\
Let us see an application 1-dim of the theorem: with $ x,b \in \mathbb{R} $
\begin{align*}
\langle x^2 \rangle
& = \frac{1}{Z(A, 0)} \frac{\partial^2}{\partial b^2} \int\limits_{-\infty}^{\infty} x^2 \hspace{1mm} e^{-\frac{A}{2} x + bx} dx \bigg \vert_{b=0} \\
& = \frac{\partial^2}{\partial b^2} \hspace{1mm} e^{ \frac{b^2}{2A} } \bigg \vert_{b=0} = \frac{\partial}{\partial b} \left[ \frac{b}{A} e^{ \frac{b^2}{2A} } \right]_{b=0} = \frac{1}{A}
\end{align*}
\begin{align*}
\langle x^4 \rangle & = \frac{1}{Z(A, 0)} \parder{b}{4} \int\limits_{-\infty}^{\infty} x^4 \hspace{1mm} e^{-\frac{A}{2} x + bx} dx \bigg \vert_{b=0} = \parder{b}{4} e^{\frac{b^2}{2A}} \bigg\vert_{b=0} = \parder{b}{3} \left[ \frac{b}{A} e^{ \frac{b^2}{2A} } \right]_{b=0} \\
& = \parder{b}{2} \left[ \frac{1}{A} e^{ \frac{b^2}{2A} } + \frac{b}{A} \cdot \frac{b}{A} e^{ \frac{b^2}{2A} } \right]_{b=0} = \partiald{b} \left[ \frac{1}{A} \cdot \frac{b}{A} e^{ \frac{b^2}{2A} } + 2 \frac{b}{A^2} e^{ \frac{b^2}{2A} } + \frac{b^2}{A^2} \cdot \frac{b}{A} e^{ \frac{b^2}{2A} } \right]_{b=o} \\
& = \left[ \frac{1}{A^2} e^{ \frac{b^2}{2A} } + \frac{b}{A^2} \cdot \frac{b}{A} e^{ \frac{b^2}{2A} } + \frac{2}{A^2} e^{ \frac{b^2}{2A} } + \frac{2b}{A^2} \cdot \frac{b}{A} e^{ \frac{b^2}{2A} } + \frac{3b^2}{A^3} e^{ \frac{b^2}{2A} } + \frac{b^3}{A^3} \cdot \frac{b}{A} e^{ \frac{b^2}{2A} } \right]_{b=0} \\
& = \frac{3}{A^2}
\end{align*}
Remembering that $ \langle x^2 \rangle = \frac{1}{A} $ we can write:
$$ \langle x^4 \rangle = 3 \langle x^2 \rangle^2 $$
Using Wick's theorem we can rewrite this last result:
With $l=4$ and A as following:
$$ A = \left(
\begin{matrix}
1 & & & \\
& 1 & & \\
& & 1 & \\
& & & 1
\end{matrix}
\right) $$
we can write:
\begin{align*}
\langle x_1 x_2 x_3 x_4 \rangle & = \langle x_1 x_2 \rangle + \langle x_3 x_4 \rangle + \langle x_1 x_3 \rangle \langle x_2 x_4 \rangle + \langle x_1 x_4 \rangle \langle x_2 x_3 \rangle \\
\end{align*}
Now come back to the 1-dim case, using $ x = x_{1,2,3,4} $, obtaining:
$$ \langle x^4 \rangle = \langle x^2 \rangle \cdot \langle x^2 \rangle \cdot 3 = 3 \langle x^2 \rangle^2 $$
which is the same result as before.}

\chapter{Path integrals}
The most captivating feature of the path-integral technique is that it provides a unified approach to solving problems in different branches of physics. For example in the formalism of the stochastic processes we use: $W(x,t|_{0},0)=\sum_{trajectory: \; x_0^t\rightarrow x}e^{-F(trajectory)}$, but this formulation come up from the QM where the probability become the amplitude $K(x,t|_{0},0)=\sum_{trajectory: \; x_0^t\rightarrow x}e^{\frac{2\pi iS}{h}(trajectory)}$ where $S=\int L dt$. \\
This formulation is not mathematically rigorous but the results are correct, since there is always the possibility to formulate it in a proper way but it will take us too much time.\\
In this kind of sum we use the concept of configuration, which means to assign to every particles of the system, we decide to study, their position and momentum. This will allow us to make averages in this way:$<A>=\sum_{config}e^{\beta E(config)}A$
\section{Diffusion}

\end{document}
